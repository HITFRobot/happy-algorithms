### LDA线性判别的思想

LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？ 我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。

在推导LDA的公式之前，需要掌握Rayleigh quotient的概念

### Rayleigh quotient

- Definition of rayleigh quotient

  rayleigh的重要性质就是它的最大值与最小值分别对应矩阵A的最大特征值与最小特征值

  ​	$$R(A, x) = \frac{x^HAx}{x^Hx}$$

- Definition of genralized rayleigh quotient

  广义rayleigh quotient的重要性质就是它的最大值与最小值分别对应矩阵B-1A的最大特征值与最小特征值

  ​	$$R(A, B, x) = \frac{x^HAx}{x^HBx}$$

### 二分类LDA的原理

假设我们的数据集D = {($x_1​$,$y_1​$),...,($x_m​$,$y_m​$)}，其中其中任意样本$x_i​$为n维向量，$y_i​$∈{0,1}。我们定义$N_j​$(j=0,1)为第j类样本的个数，$X_j​$(j=0,1)为第j类样本的集合，而$μ_j​$(j=0,1)为第j类样本的均值向量，定义$Σ_j​$(j=0,1)为第j类样本的协方差矩阵。

则$u_j$的计算表达式为

​	$$u_j = \frac{1}{N_j}\sum_{x∈X_j} x (j=0,1)$$

$Σ_j$的计算表达式为

​	$$\sum{j} = \sum_{x∈X_j} (x-u_j)(x-u_j)^T (j=0, 1)$$

由于这是一个二分类问题，因此，我们只需要把所有的数据投影到一条直线上即可。假设我们的投影直线是向量w,则对任意一个样本$x_i$,它的投影为$w^Tx_i$,对于我们的两个类别的中心点$μ_0,μ_1$,投影为$w^Tμ_0和w^Tμ_1$。由于LDA需要让不同类别的数据的类别中心之间的距离尽可能的大，也就是我们要最大化$||w^Tμ_0−w^Tμ_1||^2$,同时我们希望同一种类别数据的投影点尽可能的接近，也就是要同类样本投影点的协方差$w^TΣ_0w$和$w^TΣ_1w$尽可能的小，即最小化$w^TΣ_0w+w^TΣ_1w$。综上所述，我们的优化目标为：

​	$$arg(w) max J(w) = \frac{||w^Tμ_0−w^Tμ_1||^2}{w^TΣ_0w+w^TΣ_1w}=\frac{w^T(u_0-u_1)(u_0-u_1)^Tw}{w^T(\sum_0 +\sum_1)w}$$

定义类内散度矩阵$S_w$为：

​	$$S_w = \sum_0+\sum_1$$

定义类间散度矩阵$S_b$为：

​	$$S_b = (u_0-u_1)(u_0-u_1)^T$$

优化目标为：

​	$$arg(w) max J(w) = \frac{w^TS_bw}{w^TS_ww}$$

根据Rayleigh qutoient的性质，我们可知J(w) 最大值为矩阵$S^{-1}_wS_b$的最大特征值，而对应的w为最大特征值对应的特征向量!

### 多分类LDA原理

假设数据集D={($x_1$,$y_1$),. . .,($x_m$,$y_m$)}，$y_i$∈{$C_1$,$C_2$,...,$C_k$}。我们定义$N_j(j=1,2...k)$为第j类样本的个数，$X_j(j=1,2...k)$为第j类样本的集合，而$μ_j(j=1,2...k)$为第j类样本的均值向量，定义$Σ_j(j=1,2...k)$为第j类样本的协方差矩阵。在二类LDA里面定义的公式可以很容易的类推到多类LDA。

由于我们是多类向低维投影，则此时投影到的低维空间就不是一条直线，而是一个超平面了。假设我们投影到的低维空间的维度为d，对应的基向量为$(w_1,w_2,...w_d)$，基向量组成的矩阵为W, 它是一个m×d的矩阵。

此时我们的优化目标应该可以变成为:

​							$$\frac{W^TS_bW}{W^TS_wW}$$	

其中$S_b = \sum_{j=1}^k N_j(u_j-u)(u_j-u)^T$,$u$为所有样本的均值向量。

$$S_w = \sum_{j=1}^k S_{wj} = \sum_{j=1}^k \sum_{x∈X_j} (x-u_j)(x-u_j)^T$$

这时会出现一个问题，此时分子分母都是矩阵，无法当作标量函数来进行优化，此时我们可以用一些别的目标函数来代替

常见的LDA多类优化目标函数定义为

​			$$arg(W) max J(W) =  \frac{\prod_{diag} W^TS_bW}{\prod_{diag} W^TS_wW}$$

$\prod_{diag} A$为A的主对角元素乘积

此时J(W)的可以转化为

​			 $$J(W) = \prod_{i=1}^d \frac{w{_i}{^T}S_bw_j}{w{_i}{^T}S_ww_j}$$ 

上述式子的最大值就是广义rayleigh quotient，此时W是$S^{-1}_wS_b$的前d大个特征值对应的特征向量所组合成的矩阵

根据矩阵的秩的定义，投影矩阵W能够降到的维度d的最大值为k-1

### LDA的算法流程

input : 数据集D = {($x_1$,$y_1$),...($x_m$,$y_m$)}，$y_i$ ∈ {{$C_1$,$C_2$,...,$C_k$}}，降到的维度为d

output: 降维后的数据集

1. 计算类内散度矩阵$S_w$
2. 计算类间散度矩阵$S_b$
3. 计算矩阵$S^{-1}_wS_b$
4. 计算前d大的特征值对应的特征向量，得到投影矩阵W
5. 输出新的样本集

一个常见的LDA分类基本思想是假设各个类别的样本数据符合高斯分布，这样利用LDA进行投影后，可以利用极大似然估计计算各个类别投影数据的均值和方差，进而得到该类别高斯分布的概率密度函数。当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别带入各个类别的高斯分布概率密度函数，计算它属于这个类别的概率，最大的概率对应的类别即为预测类别。

### LDA vs PCA

**相同点**

1. 二者到都可以对数据进行降维
2. 二者在降维时都使用了矩阵分解的思想

**不同点**

1. LDA是有监督的降维，PCA是无监督降维
2. LDA最少降到k-1维，而PCA降维无限制
3. LDA除了降维，还可以分类
4. LDA选择分类性能最好的投影方向，而PCA选择投影样本点具有最大方差的方向















